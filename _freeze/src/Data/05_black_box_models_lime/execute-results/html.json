{
  "hash": "80c4156b495294b0b6a8f8fea1d5cac3",
  "result": {
    "markdown": "---\ntitle: \"Explaining Black-Box Models With LIME\"\nauthor: \"Christian SÃ¼hl\"\noutput:\n  html_document:\n    toc: TRUE\n    theme: united\n---\n\n\n# Business case\n\n::: {.cell hash='05_black_box_models_lime_cache/html/unnamed-chunk-1_3d15a8c137c1cf88ab513917861de801'}\n\n```{.r .cell-code}\n# LIME FEATURE EXPLANATION ----\n\n# 1. Setup ----\n\n# Load Libraries \n\nlibrary(h2o)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n----------------------------------------------------------------------\n\nYour next step is to start H2O:\n    > h2o.init()\n\nFor H2O package documentation, ask for help:\n    > ??h2o\n\nAfter starting H2O, you can use the Web UI at http://localhost:54321\nFor more information visit https://docs.h2o.ai\n\n----------------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'h2o'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    cor, sd, var\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n    colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n    log10, log1p, log2, round, signif, trunc\n```\n:::\n\n```{.r .cell-code}\nlibrary(recipes)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: dplyr\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'recipes'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:stats':\n\n    step\n```\n:::\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v readr     2.1.4\nv ggplot2   3.4.2     v stringr   1.5.0\nv lubridate 1.9.2     v tibble    3.2.1\nv purrr     1.0.1     v tidyr     1.3.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx lubridate::day()   masks h2o::day()\nx dplyr::filter()    masks stats::filter()\nx stringr::fixed()   masks recipes::fixed()\nx lubridate::hour()  masks h2o::hour()\nx dplyr::lag()       masks stats::lag()\nx lubridate::month() masks h2o::month()\nx lubridate::week()  masks h2o::week()\nx lubridate::year()  masks h2o::year()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyquant)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: PerformanceAnalytics\nLoading required package: xts\nLoading required package: zoo\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\nAttaching package: 'xts'\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\n\nAttaching package: 'PerformanceAnalytics'\n\nThe following object is masked from 'package:graphics':\n\n    legend\n\nLoading required package: quantmod\nLoading required package: TTR\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n```\n:::\n\n```{.r .cell-code}\nlibrary(lime)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lime'\n\nThe following object is masked from 'package:dplyr':\n\n    explain\n```\n:::\n\n```{.r .cell-code}\nlibrary(rsample)\n\n# Load Data\nemployee_attrition_tbl <- read_csv(\"datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 1470 Columns: 35\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\ndbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndefinitions_raw_tbl    <- read_excel(\"data_definitions.xlsx\", sheet = 1, col_names = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNew names:\n* `` -> `...1`\n* `` -> `...2`\n```\n:::\n\n```{.r .cell-code}\nprocess_hr_data_readable <- function(data, definitions_tbl) {\n  \n  definitions_list <- definitions_tbl %>%\n    fill(...1, .direction = \"down\") %>%\n    filter(!is.na(...2)) %>%\n    separate(...2, into = c(\"key\", \"value\"), sep = \" '\", remove = TRUE) %>%\n    rename(column_name = ...1) %>%\n    mutate(key = as.numeric(key)) %>%\n    mutate(value = value %>% str_replace(pattern = \"'\", replacement = \"\")) %>%\n    split(.$column_name) %>%\n    map(~ select(., -column_name)) %>%\n    map(~ mutate(., value = as_factor(value))) \n  \n  for (i in seq_along(definitions_list)) {\n    list_name <- names(definitions_list)[i]\n    colnames(definitions_list[[i]]) <- c(list_name, paste0(list_name, \"_value\"))\n  }\n  \n  data_merged_tbl <- list(HR_Data = data) %>%\n    append(definitions_list, after = 1) %>%\n    reduce(left_join) %>%\n    select(-one_of(names(definitions_list))) %>%\n    set_names(str_replace_all(names(.), pattern = \"_value\", \n                              replacement = \"\")) %>%\n    select(sort(names(.))) %>%\n    mutate_if(is.character, as.factor) %>%\n    mutate(\n      BusinessTravel = BusinessTravel %>% fct_relevel(\"Non-Travel\", \n                                                      \"Travel_Rarely\", \n                                                      \"Travel_Frequently\"),\n      MaritalStatus  = MaritalStatus %>% fct_relevel(\"Single\", \n                                                     \"Married\", \n                                                     \"Divorced\")\n    )\n  \n  return(data_merged_tbl)\n  \n}\n\nemployee_attrition_readable_tbl <- process_hr_data_readable(employee_attrition_tbl, definitions_raw_tbl)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(Education)`\nJoining with `by = join_by(EnvironmentSatisfaction)`\nJoining with `by = join_by(JobInvolvement)`\nJoining with `by = join_by(JobSatisfaction)`\nJoining with `by = join_by(PerformanceRating)`\nJoining with `by = join_by(RelationshipSatisfaction)`\nJoining with `by = join_by(WorkLifeBalance)`\n```\n:::\n\n```{.r .cell-code}\n# Split into test and train\nset.seed(seed = 1113)\nsplit_obj <- rsample::initial_split(employee_attrition_readable_tbl, prop = 0.85)\n\n# Assign training and test data\ntrain_readable_tbl <- training(split_obj)\ntest_readable_tbl  <- testing(split_obj)\n\n# ML Preprocessing Recipe \nrecipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%\n                step_zv(all_predictors()) %>%\n                step_mutate_at(c(\"JobLevel\", \"StockOptionLevel\"), fn = as.factor) %>% \n                prep()\n\nrecipe_obj\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n-- Recipe ----------------------------------------------------------------------\n\n-- Inputs \nNumber of variables by role\noutcome:    1\npredictor: 34\n\n-- Training information \nTraining data contained 1249 data points and no incomplete rows.\n\n-- Operations \n* Zero variance filter removed: EmployeeCount, Over18, StandardHours | Trained\n* Variable mutation for: JobLevel, StockOptionLevel | Trained\n```\n:::\n\n```{.r .cell-code}\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)\n\n# 2. Models ----\n\nh2o.init()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         7 hours 15 minutes \n    H2O cluster timezone:       Europe/Berlin \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.40.0.1 \n    H2O cluster version age:    3 months and 12 days \n    H2O cluster name:           H2O_started_from_R_Chris_yuh446 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   2.16 GB \n    H2O cluster total cores:    12 \n    H2O cluster allowed cores:  12 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.0.5 (2021-03-31) \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in h2o.clusterInfo(): \nYour H2O cluster version is (3 months and 12 days) old. There may be a newer version available.\nPlease download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n```\n:::\n\n```{.r .cell-code}\nautoml_leader <- h2o.loadModel(\"04_Modeling/h20_models/DRF_1_AutoML_28_20230520_183822\")\nautoml_leader\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel Details:\n==============\n\nH2OBinomialModel: drf\nModel ID:  DRF_1_AutoML_28_20230520_183822 \nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              46                       46               66692        11\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1        17   13.32609         88        129   110.36957\n\n\nH2OBinomialMetrics: drf\n** Reported on training data. **\n** Metrics reported on Out-Of-Bag training samples **\n\nMSE:  0.1091607\nRMSE:  0.3303948\nLogLoss:  0.6215371\nMean Per-Class Error:  0.2860322\nAUC:  0.7497888\nAUCPR:  0.3857798\nGini:  0.4995777\nR^2:  0.1228901\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error       Rate\nNo     735 174 0.191419   =174/909\nYes     59  96 0.380645    =59/155\nTotals 794 270 0.218985  =233/1064\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.227273   0.451765  65\n2                       max f2  0.187500   0.541922  74\n3                 max f0point5  0.320000   0.444596  46\n4                 max accuracy  0.538462   0.859962  16\n5                max precision  0.800000   1.000000   0\n6                   max recall  0.000000   1.000000 116\n7              max specificity  0.800000   1.000000   0\n8             max absolute_mcc  0.300000   0.347232  51\n9   max min_per_class_accuracy  0.181818   0.690323  76\n10 max mean_per_class_accuracy  0.227273   0.713968  65\n11                     max tns  0.800000 909.000000   0\n12                     max fns  0.800000 154.000000   0\n13                     max fps  0.000000 909.000000 116\n14                     max tps  0.000000 155.000000 116\n15                     max tnr  0.800000   1.000000   0\n16                     max fnr  0.800000   0.993548   0\n17                     max fpr  0.000000   1.000000 116\n18                     max tpr  0.000000   1.000000 116\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nH2OBinomialMetrics: drf\n** Reported on validation data. **\n\nMSE:  0.1271733\nRMSE:  0.3566136\nLogLoss:  0.4095014\nMean Per-Class Error:  0.2429288\nAUC:  0.7916219\nAUCPR:  0.5760425\nGini:  0.5832438\nR^2:  0.220819\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error     Rate\nNo     122  25 0.170068  =25/147\nYes     12  26 0.315789   =12/38\nTotals 134  51 0.200000  =37/185\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.260870   0.584270  18\n2                       max f2  0.173913   0.657895  22\n3                 max f0point5  0.347826   0.601266  14\n4                 max accuracy  0.347826   0.837838  14\n5                max precision  0.782609   1.000000   0\n6                   max recall  0.043478   1.000000  28\n7              max specificity  0.782609   1.000000   0\n8             max absolute_mcc  0.347826   0.466001  14\n9   max min_per_class_accuracy  0.195652   0.734694  21\n10 max mean_per_class_accuracy  0.260870   0.757071  18\n11                     max tns  0.782609 147.000000   0\n12                     max fns  0.782609  37.000000   0\n13                     max fps  0.000000 147.000000  30\n14                     max tps  0.043478  38.000000  28\n15                     max tnr  0.782609   1.000000   0\n16                     max fnr  0.782609   0.973684   0\n17                     max fpr  0.000000   1.000000  30\n18                     max tpr  0.043478   1.000000  28\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nH2OBinomialMetrics: drf\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\nMSE:  0.1072195\nRMSE:  0.3274439\nLogLoss:  0.450539\nMean Per-Class Error:  0.3141701\nAUC:  0.7388303\nAUCPR:  0.4193506\nGini:  0.4776607\nR^2:  0.1384876\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error       Rate\nNo     807 102 0.112211   =102/909\nYes     80  75 0.516129    =80/155\nTotals 887 177 0.171053  =182/1064\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.266667   0.451807  30\n2                       max f2  0.160000   0.528338  40\n3                 max f0point5  0.433333   0.478873  17\n4                 max accuracy  0.433333   0.871241  17\n5                max precision  0.760000   1.000000   0\n6                   max recall  0.000000   1.000000  52\n7              max specificity  0.760000   1.000000   0\n8             max absolute_mcc  0.266667   0.352083  30\n9   max min_per_class_accuracy  0.160000   0.657866  40\n10 max mean_per_class_accuracy  0.240000   0.689159  33\n11                     max tns  0.760000 909.000000   0\n12                     max fns  0.760000 154.000000   0\n13                     max fps  0.000000 909.000000  52\n14                     max tps  0.000000 155.000000  52\n15                     max tnr  0.760000   1.000000   0\n16                     max fnr  0.760000   0.993548   0\n17                     max fpr  0.000000   1.000000  52\n18                     max tpr  0.000000   1.000000  52\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nCross-Validation Metrics Summary: \n                             mean       sd cv_1_valid cv_2_valid cv_3_valid\naccuracy                 0.843046 0.021398   0.816901   0.863850   0.826291\nauc                      0.738616 0.047452   0.739412   0.777769   0.752875\nerr                      0.156954 0.021398   0.183099   0.136150   0.173709\nerr_count               33.400000 4.560702  39.000000  29.000000  37.000000\nf0point5                 0.471185 0.059526   0.396825   0.559701   0.467033\nf1                       0.464520 0.041745   0.434783   0.508475   0.478873\nf2                       0.467599 0.071645   0.480769   0.465839   0.491329\nlift_top_group           4.651961 1.529051   4.896552   4.698529   4.176471\nlogloss                  0.450165 0.137775   0.353070   0.365467   0.376165\nmax_per_class_error      0.525282 0.100763   0.482759   0.558824   0.500000\nmcc                      0.382730 0.042299   0.334891   0.438458   0.375354\nmean_per_class_accuracy  0.690725 0.034587   0.690686   0.692655   0.694134\nmean_per_class_error     0.309275 0.034587   0.309314   0.307345   0.305866\nmse                      0.106708 0.006655   0.104813   0.109384   0.112639\npr_auc                   0.426490 0.059228   0.361906   0.486945   0.470138\nprecision                0.482214 0.093549   0.375000   0.600000   0.459459\nr2                       0.139951 0.032814   0.108833   0.184581   0.160314\nrecall                   0.474718 0.100763   0.517241   0.441176   0.500000\nrmse                     0.326531 0.010325   0.323749   0.330732   0.335618\nspecificity              0.906732 0.040756   0.864130   0.944134   0.888268\n                        cv_4_valid cv_5_valid\naccuracy                  0.863850   0.844340\nauc                       0.657657   0.765365\nerr                       0.136150   0.155660\nerr_count                29.000000  33.000000\nf0point5                  0.485437   0.446927\nf1                        0.408163   0.492308\nf2                        0.352113   0.547945\nlift_top_group            6.870968   2.617284\nlogloss                   0.680863   0.475258\nmax_per_class_error       0.677419   0.407407\nmcc                       0.353250   0.411700\nmean_per_class_accuracy   0.639312   0.736837\nmean_per_class_error      0.360688   0.263163\nmse                       0.110725   0.095979\npr_auc                    0.448680   0.364780\nprecision                 0.555556   0.421053\nr2                        0.109628   0.136398\nrecall                    0.322581   0.592593\nrmse                      0.332753   0.309805\nspecificity               0.956044   0.881081\n```\n:::\n\n```{.r .cell-code}\n# 3. LIME ----\n\n# 3.1 Making Predictions ----\n\npredictions_tbl <- automl_leader %>% \n    h2o.predict(newdata = as.h2o(test_tbl)) %>%\n    as.tibble() %>%\n    bind_cols(\n        test_tbl %>%\n            select(Attrition, EmployeeNumber)\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\ni Please use `as_tibble()` instead.\ni The signature and semantics have changed, see `?as_tibble`.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\npredictions_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 221 x 5\n   predict    No    Yes Attrition EmployeeNumber\n   <fct>   <dbl>  <dbl> <fct>              <dbl>\n 1 Yes     0.478 0.522  No                    10\n 2 No      0.891 0.109  No                    16\n 3 No      0.804 0.196  No                    30\n 4 No      0.913 0.0870 Yes                   31\n 5 No      0.957 0.0435 No                    40\n 6 Yes     0.717 0.283  Yes                   42\n 7 No      0.957 0.0435 No                    46\n 8 Yes     0.609 0.391  Yes                   55\n 9 Yes     0.630 0.370  Yes                   64\n10 No      1     0      No                    77\n# i 211 more rows\n```\n:::\n\n```{.r .cell-code}\n# 3.2 Single Explanation ----\n\nexplainer <- train_tbl %>%\n    select(-Attrition) %>%\n    lime(\n        model           = automl_leader,\n        bin_continuous  = TRUE,\n        n_bins          = 4,\n        quantile_bins   = TRUE\n    )\n\nexplainer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$model\nModel Details:\n==============\n\nH2OBinomialModel: drf\nModel ID:  DRF_1_AutoML_28_20230520_183822 \nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              46                       46               66692        11\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1        17   13.32609         88        129   110.36957\n\n\nH2OBinomialMetrics: drf\n** Reported on training data. **\n** Metrics reported on Out-Of-Bag training samples **\n\nMSE:  0.1091607\nRMSE:  0.3303948\nLogLoss:  0.6215371\nMean Per-Class Error:  0.2860322\nAUC:  0.7497888\nAUCPR:  0.3857798\nGini:  0.4995777\nR^2:  0.1228901\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error       Rate\nNo     735 174 0.191419   =174/909\nYes     59  96 0.380645    =59/155\nTotals 794 270 0.218985  =233/1064\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.227273   0.451765  65\n2                       max f2  0.187500   0.541922  74\n3                 max f0point5  0.320000   0.444596  46\n4                 max accuracy  0.538462   0.859962  16\n5                max precision  0.800000   1.000000   0\n6                   max recall  0.000000   1.000000 116\n7              max specificity  0.800000   1.000000   0\n8             max absolute_mcc  0.300000   0.347232  51\n9   max min_per_class_accuracy  0.181818   0.690323  76\n10 max mean_per_class_accuracy  0.227273   0.713968  65\n11                     max tns  0.800000 909.000000   0\n12                     max fns  0.800000 154.000000   0\n13                     max fps  0.000000 909.000000 116\n14                     max tps  0.000000 155.000000 116\n15                     max tnr  0.800000   1.000000   0\n16                     max fnr  0.800000   0.993548   0\n17                     max fpr  0.000000   1.000000 116\n18                     max tpr  0.000000   1.000000 116\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nH2OBinomialMetrics: drf\n** Reported on validation data. **\n\nMSE:  0.1271733\nRMSE:  0.3566136\nLogLoss:  0.4095014\nMean Per-Class Error:  0.2429288\nAUC:  0.7916219\nAUCPR:  0.5760425\nGini:  0.5832438\nR^2:  0.220819\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error     Rate\nNo     122  25 0.170068  =25/147\nYes     12  26 0.315789   =12/38\nTotals 134  51 0.200000  =37/185\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.260870   0.584270  18\n2                       max f2  0.173913   0.657895  22\n3                 max f0point5  0.347826   0.601266  14\n4                 max accuracy  0.347826   0.837838  14\n5                max precision  0.782609   1.000000   0\n6                   max recall  0.043478   1.000000  28\n7              max specificity  0.782609   1.000000   0\n8             max absolute_mcc  0.347826   0.466001  14\n9   max min_per_class_accuracy  0.195652   0.734694  21\n10 max mean_per_class_accuracy  0.260870   0.757071  18\n11                     max tns  0.782609 147.000000   0\n12                     max fns  0.782609  37.000000   0\n13                     max fps  0.000000 147.000000  30\n14                     max tps  0.043478  38.000000  28\n15                     max tnr  0.782609   1.000000   0\n16                     max fnr  0.782609   0.973684   0\n17                     max fpr  0.000000   1.000000  30\n18                     max tpr  0.043478   1.000000  28\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nH2OBinomialMetrics: drf\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\nMSE:  0.1072195\nRMSE:  0.3274439\nLogLoss:  0.450539\nMean Per-Class Error:  0.3141701\nAUC:  0.7388303\nAUCPR:  0.4193506\nGini:  0.4776607\nR^2:  0.1384876\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n        No Yes    Error       Rate\nNo     807 102 0.112211   =102/909\nYes     80  75 0.516129    =80/155\nTotals 887 177 0.171053  =182/1064\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.266667   0.451807  30\n2                       max f2  0.160000   0.528338  40\n3                 max f0point5  0.433333   0.478873  17\n4                 max accuracy  0.433333   0.871241  17\n5                max precision  0.760000   1.000000   0\n6                   max recall  0.000000   1.000000  52\n7              max specificity  0.760000   1.000000   0\n8             max absolute_mcc  0.266667   0.352083  30\n9   max min_per_class_accuracy  0.160000   0.657866  40\n10 max mean_per_class_accuracy  0.240000   0.689159  33\n11                     max tns  0.760000 909.000000   0\n12                     max fns  0.760000 154.000000   0\n13                     max fps  0.000000 909.000000  52\n14                     max tps  0.000000 155.000000  52\n15                     max tnr  0.760000   1.000000   0\n16                     max fnr  0.760000   0.993548   0\n17                     max fpr  0.000000   1.000000  52\n18                     max tpr  0.000000   1.000000  52\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nCross-Validation Metrics Summary: \n                             mean       sd cv_1_valid cv_2_valid cv_3_valid\naccuracy                 0.843046 0.021398   0.816901   0.863850   0.826291\nauc                      0.738616 0.047452   0.739412   0.777769   0.752875\nerr                      0.156954 0.021398   0.183099   0.136150   0.173709\nerr_count               33.400000 4.560702  39.000000  29.000000  37.000000\nf0point5                 0.471185 0.059526   0.396825   0.559701   0.467033\nf1                       0.464520 0.041745   0.434783   0.508475   0.478873\nf2                       0.467599 0.071645   0.480769   0.465839   0.491329\nlift_top_group           4.651961 1.529051   4.896552   4.698529   4.176471\nlogloss                  0.450165 0.137775   0.353070   0.365467   0.376165\nmax_per_class_error      0.525282 0.100763   0.482759   0.558824   0.500000\nmcc                      0.382730 0.042299   0.334891   0.438458   0.375354\nmean_per_class_accuracy  0.690725 0.034587   0.690686   0.692655   0.694134\nmean_per_class_error     0.309275 0.034587   0.309314   0.307345   0.305866\nmse                      0.106708 0.006655   0.104813   0.109384   0.112639\npr_auc                   0.426490 0.059228   0.361906   0.486945   0.470138\nprecision                0.482214 0.093549   0.375000   0.600000   0.459459\nr2                       0.139951 0.032814   0.108833   0.184581   0.160314\nrecall                   0.474718 0.100763   0.517241   0.441176   0.500000\nrmse                     0.326531 0.010325   0.323749   0.330732   0.335618\nspecificity              0.906732 0.040756   0.864130   0.944134   0.888268\n                        cv_4_valid cv_5_valid\naccuracy                  0.863850   0.844340\nauc                       0.657657   0.765365\nerr                       0.136150   0.155660\nerr_count                29.000000  33.000000\nf0point5                  0.485437   0.446927\nf1                        0.408163   0.492308\nf2                        0.352113   0.547945\nlift_top_group            6.870968   2.617284\nlogloss                   0.680863   0.475258\nmax_per_class_error       0.677419   0.407407\nmcc                       0.353250   0.411700\nmean_per_class_accuracy   0.639312   0.736837\nmean_per_class_error      0.360688   0.263163\nmse                       0.110725   0.095979\npr_auc                    0.448680   0.364780\nprecision                 0.555556   0.421053\nr2                        0.109628   0.136398\nrecall                    0.322581   0.592593\nrmse                      0.332753   0.309805\nspecificity               0.956044   0.881081\n\n$preprocess\nfunction (x) \nx\n<bytecode: 0x0000000029a14d08>\n<environment: 0x0000000029a0b348>\n\n$bin_continuous\n[1] TRUE\n\n$n_bins\n[1] 4\n\n$quantile_bins\n[1] TRUE\n\n$use_density\n[1] TRUE\n\n$feature_type\n                     Age           BusinessTravel                DailyRate \n               \"numeric\"                 \"factor\"                \"numeric\" \n              Department         DistanceFromHome                Education \n                \"factor\"                \"numeric\"                 \"factor\" \n          EducationField           EmployeeNumber  EnvironmentSatisfaction \n                \"factor\"                \"numeric\"                 \"factor\" \n                  Gender               HourlyRate           JobInvolvement \n                \"factor\"                \"numeric\"                 \"factor\" \n                JobLevel                  JobRole          JobSatisfaction \n                \"factor\"                 \"factor\"                 \"factor\" \n           MaritalStatus            MonthlyIncome              MonthlyRate \n                \"factor\"                \"numeric\"                \"numeric\" \n      NumCompaniesWorked                 OverTime        PercentSalaryHike \n               \"numeric\"                 \"factor\"                \"numeric\" \n       PerformanceRating RelationshipSatisfaction         StockOptionLevel \n                \"factor\"                 \"factor\"                 \"factor\" \n       TotalWorkingYears    TrainingTimesLastYear          WorkLifeBalance \n               \"numeric\"                \"numeric\"                 \"factor\" \n          YearsAtCompany       YearsInCurrentRole  YearsSinceLastPromotion \n               \"numeric\"                \"numeric\"                \"numeric\" \n    YearsWithCurrManager \n               \"numeric\" \n\n$bin_cuts\n$bin_cuts$Age\n  0%  25%  50%  75% 100% \n  18   30   36   43   60 \n\n$bin_cuts$BusinessTravel\nNULL\n\n$bin_cuts$DailyRate\n  0%  25%  50%  75% 100% \n 102  465  797 1147 1499 \n\n$bin_cuts$Department\nNULL\n\n$bin_cuts$DistanceFromHome\n  0%  25%  50%  75% 100% \n   1    2    7   14   29 \n\n$bin_cuts$Education\nNULL\n\n$bin_cuts$EducationField\nNULL\n\n$bin_cuts$EmployeeNumber\n  0%  25%  50%  75% 100% \n   1  511 1040 1573 2065 \n\n$bin_cuts$EnvironmentSatisfaction\nNULL\n\n$bin_cuts$Gender\nNULL\n\n$bin_cuts$HourlyRate\n  0%  25%  50%  75% 100% \n  30   49   66   83  100 \n\n$bin_cuts$JobInvolvement\nNULL\n\n$bin_cuts$JobLevel\nNULL\n\n$bin_cuts$JobRole\nNULL\n\n$bin_cuts$JobSatisfaction\nNULL\n\n$bin_cuts$MaritalStatus\nNULL\n\n$bin_cuts$MonthlyIncome\n   0%   25%   50%   75%  100% \n 1051  2929  4908  8474 19999 \n\n$bin_cuts$MonthlyRate\n   0%   25%   50%   75%  100% \n 2094  8423 14470 20689 26968 \n\n$bin_cuts$NumCompaniesWorked\n  0%  25%  50%  75% 100% \n   0    1    2    4    9 \n\n$bin_cuts$OverTime\nNULL\n\n$bin_cuts$PercentSalaryHike\n  0%  25%  50%  75% 100% \n  11   12   14   18   25 \n\n$bin_cuts$PerformanceRating\nNULL\n\n$bin_cuts$RelationshipSatisfaction\nNULL\n\n$bin_cuts$StockOptionLevel\nNULL\n\n$bin_cuts$TotalWorkingYears\n  0%  25%  50%  75% 100% \n   0    6   10   15   38 \n\n$bin_cuts$TrainingTimesLastYear\n  0%  25%  50% 100% \n   0    2    3    6 \n\n$bin_cuts$WorkLifeBalance\nNULL\n\n$bin_cuts$YearsAtCompany\n  0%  25%  50%  75% 100% \n   0    3    5    9   37 \n\n$bin_cuts$YearsInCurrentRole\n  0%  25%  50%  75% 100% \n   0    2    3    7   18 \n\n$bin_cuts$YearsSinceLastPromotion\n  0%  50%  75% 100% \n   0    1    2   15 \n\n$bin_cuts$YearsWithCurrManager\n  0%  25%  50%  75% 100% \n   0    2    3    7   17 \n\n\n$feature_distribution\n$feature_distribution$Age\n\n        1         2         3         4 \n0.2602082 0.2834267 0.2217774 0.2345877 \n\n$feature_distribution$BusinessTravel\n\n       Non-Travel     Travel_Rarely Travel_Frequently \n        0.1000801         0.7181745         0.1817454 \n\n$feature_distribution$DailyRate\n\n        1         2         3         4 \n0.2514011 0.2489992 0.2497998 0.2497998 \n\n$feature_distribution$Department\n\n       Human Resources Research & Development                  Sales \n            0.04323459             0.65092074             0.30584468 \n\n$feature_distribution$DistanceFromHome\n\n        1         2         3         4 \n0.2954363 0.2369896 0.2241793 0.2433947 \n\n$feature_distribution$Education\n\nBelow College       College      Bachelor        Master        Doctor \n   0.11689351    0.18895116    0.38510809    0.27461970    0.03442754 \n\n$feature_distribution$EducationField\n\n Human Resources    Life Sciences        Marketing          Medical \n      0.01761409       0.41793435       0.10888711       0.31144916 \n           Other Technical Degree \n      0.05444355       0.08967174 \n\n$feature_distribution$EmployeeNumber\n\n        1         2         3         4 \n0.2506005 0.2497998 0.2497998 0.2497998 \n\n$feature_distribution$EnvironmentSatisfaction\n\n      Low    Medium      High Very High \n0.1913531 0.1961569 0.3018415 0.3106485 \n\n$feature_distribution$Gender\n\n   Female      Male \n0.4123299 0.5876701 \n\n$feature_distribution$HourlyRate\n\n        1         2         3         4 \n0.2618094 0.2473979 0.2449960 0.2457966 \n\n$feature_distribution$JobInvolvement\n\n       Low     Medium       High  Very High \n0.05684548 0.25780624 0.58927142 0.09607686 \n\n$feature_distribution$JobLevel\n\n         1          2          3          4          5 \n0.36829464 0.36509207 0.14651721 0.07526021 0.04483587 \n\n$feature_distribution$JobRole\n\nHealthcare Representative           Human Resources     Laboratory Technician \n               0.08646918                0.03682946                0.18174540 \n                  Manager    Manufacturing Director         Research Director \n               0.06885508                0.09927942                0.05924740 \n       Research Scientist           Sales Executive      Sales Representative \n               0.18654924                0.22337870                0.05764612 \n\n$feature_distribution$JobSatisfaction\n\n      Low    Medium      High Very High \n0.1873499 0.1985588 0.3018415 0.3122498 \n\n$feature_distribution$MaritalStatus\n\n   Single   Married  Divorced \n0.3306645 0.4571657 0.2121697 \n\n$feature_distribution$MonthlyIncome\n\n        1         2         3         4 \n0.2506005 0.2497998 0.2497998 0.2497998 \n\n$feature_distribution$MonthlyRate\n\n        1         2         3         4 \n0.2506005 0.2497998 0.2497998 0.2497998 \n\n$feature_distribution$NumCompaniesWorked\n\n         1          2          3          4 \n0.48118495 0.09927942 0.20496397 0.21457166 \n\n$feature_distribution$OverTime\n\n       No       Yes \n0.7165733 0.2834267 \n\n$feature_distribution$PercentSalaryHike\n\n        1         2         3         4 \n0.2866293 0.2738191 0.2289832 0.2105685 \n\n$feature_distribution$PerformanceRating\n\n        Low        Good   Excellent Outstanding \n  0.0000000   0.0000000   0.8414732   0.1585268 \n\n$feature_distribution$RelationshipSatisfaction\n\n      Low    Medium      High Very High \n0.1889512 0.2161729 0.3018415 0.2930344 \n\n$feature_distribution$StockOptionLevel\n\n         0          1          2          3 \n0.43554844 0.40592474 0.10168135 0.05684548 \n\n$feature_distribution$TotalWorkingYears\n\n        1         2         3         4 \n0.3050440 0.3306645 0.1224980 0.2417934 \n\n$feature_distribution$TrainingTimesLastYear\n\n        1         2         3 \n0.4603683 0.3306645 0.2089672 \n\n$feature_distribution$WorkLifeBalance\n\n       Bad       Good     Better       Best \n0.05204163 0.22497998 0.61889512 0.10408327 \n\n$feature_distribution$YearsAtCompany\n\n        1         2         3         4 \n0.3226581 0.2137710 0.2217774 0.2417934 \n\n$feature_distribution$YearsInCurrentRole\n\n         1          2          3          4 \n0.46757406 0.08726982 0.27542034 0.16973579 \n\n$feature_distribution$YearsSinceLastPromotion\n\n        1         2         3 \n0.6413131 0.1120897 0.2465973 \n\n$feature_distribution$YearsWithCurrManager\n\n         1          2          3          4 \n0.46357086 0.09767814 0.25300240 0.18574860 \n\n\nattr(,\"class\")\n[1] \"data_frame_explainer\" \"explainer\"            \"list\"                \n```\n:::\n\n```{.r .cell-code}\nexplanation <- test_tbl %>%\n    slice(1) %>%\n    select(-Attrition) %>%\n    lime::explain(\n    \n        # Pass our explainer object\n        explainer = explainer,\n        # Because it is a binary classification model: 1\n        n_labels   = 1,\n        # number of features to be returned\n        n_features = 8,\n        # number of localized linear models\n        n_permutations = 5000,\n        # Let's start with 1\n        kernel_width   = 1\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\nexplanation\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 13\n  model_type    case  label label_prob model_r2 model_intercept model_prediction\n  <chr>         <chr> <chr>      <dbl>    <dbl>           <dbl>            <dbl>\n1 classificati~ 1     Yes        0.522    0.333           0.132            0.408\n2 classificati~ 1     Yes        0.522    0.333           0.132            0.408\n3 classificati~ 1     Yes        0.522    0.333           0.132            0.408\n4 classificati~ 1     Yes        0.522    0.333           0.132            0.408\n5 classificati~ 1     Yes        0.522    0.333           0.132            0.408\n6 classificati~ 1     Yes        0.522    0.333           0.132            0.408\n7 classificati~ 1     Yes        0.522    0.333           0.132            0.408\n8 classificati~ 1     Yes        0.522    0.333           0.132            0.408\n# i 6 more variables: feature <chr>, feature_value <dbl>, feature_weight <dbl>,\n#   feature_desc <chr>, data <list>, prediction <list>\n```\n:::\n\n```{.r .cell-code}\nexplanation %>%\n    as.tibble() %>%\n    select(feature:prediction) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 6\n  feature    feature_value feature_weight feature_desc data         prediction  \n  <chr>              <dbl>          <dbl> <chr>        <list>       <list>      \n1 OverTime               2         0.110  OverTime = ~ <named list> <named list>\n2 MonthlyIn~          2670         0.0460 MonthlyInco~ <named list> <named list>\n3 JobSatisf~             1         0.0385 JobSatisfac~ <named list> <named list>\n4 JobRole                3         0.0284 JobRole = L~ <named list> <named list>\n5 YearsAtCo~             1         0.0265 YearsAtComp~ <named list> <named list>\n6 StockOpti~             4         0.0245 StockOption~ <named list> <named list>\n7 JobLevel               1         0.0237 JobLevel = 1 <named list> <named list>\n8 BusinessT~             2        -0.0210 BusinessTra~ <named list> <named list>\n```\n:::\n\n```{.r .cell-code}\ng <- plot_features(explanation = explanation, ncol = 1)\n\n# 3.3 Multiple Explanations ----\n\nexplanation <- test_tbl %>%\n    slice(1:20) %>%\n    select(-Attrition) %>%\n    lime::explain(\n        explainer = explainer,\n        n_labels   = 1,\n        n_features = 8,\n        n_permutations = 5000,\n        kernel_width   = 0.5\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\nexplanation %>%\n    as.tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 160 x 13\n   model_type   case  label label_prob model_r2 model_intercept model_prediction\n   <chr>        <chr> <chr>      <dbl>    <dbl>           <dbl>            <dbl>\n 1 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 2 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 3 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 4 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 5 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 6 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 7 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 8 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 9 classificat~ 2     No         0.891    0.329           0.747            0.782\n10 classificat~ 2     No         0.891    0.329           0.747            0.782\n# i 150 more rows\n# i 6 more variables: feature <chr>, feature_value <dbl>, feature_weight <dbl>,\n#   feature_desc <chr>, data <list>, prediction <list>\n```\n:::\n\n```{.r .cell-code}\nplot_features(explanation, ncol = 4)\n```\n\n::: {.cell-output-display}\n![](05_black_box_models_lime_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot_explanations(explanation)\n```\n\n::: {.cell-output-display}\n![](05_black_box_models_lime_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n# Challenge\n## Part 1\n\n::: {.cell hash='05_black_box_models_lime_cache/html/unnamed-chunk-2_f0553d0bc02de8234ef879fa73eac230'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nexplanation %>% as.tibble()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\ni Please use `as_tibble()` instead.\ni The signature and semantics have changed, see `?as_tibble`.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 160 x 13\n   model_type   case  label label_prob model_r2 model_intercept model_prediction\n   <chr>        <chr> <chr>      <dbl>    <dbl>           <dbl>            <dbl>\n 1 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 2 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 3 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 4 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 5 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 6 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 7 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 8 classificat~ 1     Yes        0.522    0.321           0.134            0.392\n 9 classificat~ 2     No         0.891    0.329           0.747            0.782\n10 classificat~ 2     No         0.891    0.329           0.747            0.782\n# i 150 more rows\n# i 6 more variables: feature <chr>, feature_value <dbl>, feature_weight <dbl>,\n#   feature_desc <chr>, data <list>, prediction <list>\n```\n:::\n\n```{.r .cell-code}\n# Reference\nexplanation %>% filter(case == 1) %>% plot_features()\n```\n\n::: {.cell-output-display}\n![](05_black_box_models_lime_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# My version\nexplanation %>%\n    filter(case == 1) %>%\n    ggplot(aes(x = fct_reorder(feature,abs(feature_weight)),y = feature_weight,fill = feature_weight < 0)) +\n    coord_flip() +\n    geom_col(just = 0.5) + \n    labs( title = paste0(\n      \"Case: \",\"1\",\"\\n\",\n      \"Label: \",filter(explanation,case == 1)[1,]$label,\"\\n\",\n      \"Probability: \",filter(explanation,case == 1)[1,]$label_prob%>%round(2),\"\\n\",\n      \"Explanation Fit: \",filter(explanation,case == 1)[1,]$model_r2%>%round(2),\"\\n\"\n      ),\n      x = \"Feature\",\n      y = \"Weight\",\n      fill = \"\"\n    ) +\n    scale_x_discrete(labels=explanation %>% filter(case == 1) %>% arrange(abs(feature_weight)) %>% .$feature_desc) +\n    theme(legend.position = \"bottom\",\n          panel.background = element_blank(),\n          panel.grid.major.x = element_line(color=\"#EBEBEB\"),\n          panel.grid.minor.x = element_line(color=\"#EBEBEB\")\n    ) +\n    scale_fill_manual(values=c(\"#4682B4\", \"#B22222\"),labels=c(\"Supports\",\"Contradics\"))\n```\n\n::: {.cell-output-display}\n![](05_black_box_models_lime_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\n## Part 2\n\n::: {.cell hash='05_black_box_models_lime_cache/html/unnamed-chunk-3_b9d9e96287db840c18d14a1420c6958d'}\n\n```{.r .cell-code}\n# Reference\nplot_explanations(explanation)\n```\n\n::: {.cell-output-display}\n![](05_black_box_models_lime_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# My version\nexplanation %>%\n    ggplot(aes(x = case,y = fct_reorder(feature,abs(label_prob)),fill = feature_weight)) +\n    facet_wrap(~label) + \n    geom_tile() + \n    labs(\n      x = \"Case\",\n      y = \"Feature\"\n    ) +\n    scale_y_discrete(labels=explanation%>% arrange(abs(label_prob)) %>% .$feature_desc) +\n    theme(panel.background = element_blank(),\n          panel.grid = element_blank(),\n          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)\n    ) +\n  scale_fill_gradient2('Feature \\n weight', low = '#B9362F', mid = \"white\", high = '#4B85B6')\n```\n\n::: {.cell-output-display}\n![](05_black_box_models_lime_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}